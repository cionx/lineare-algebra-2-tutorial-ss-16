%\documentclass[a4paper,10pt]{article}
\documentclass[a4paper,10pt,numbers=noenddot]{scrartcl}


\usepackage{../generalstyle}
\usepackage{specificstyle}


\title{Lösungen zu Zettel 12}
\author{Jendrik Stelzner}
\date{\today}


\begin{document}
\maketitle










\begin{remark}
  In der Vorlesung wurde anscheinend nicht definiert, was das Symbol $\oplus$ in $V \oplus V^*$ bedeuten soll.
  Wir werden dies hier ebenfalls nicht tun.
  Stattdessen geben wir den Fakt an, dass für endlich viele $K$-Vektorräume $V_1, \dotsc, V_n$ die Gleichheit
  \[
      V_1 \oplus \dotsb \oplus V_n
    = V_1 \times \dotsb \times V_n.
  \]
  gilt.
  Wir werden daher im Folgenden jeweils mit $\times$ statt mit $\oplus$ arbeiten.
\end{remark}










\addtocounter{section}{2}










\section{}


Um zu zeigen, dass $q \colon V \times V^* \to K$ mit $q(v, \varphi) = \varphi(v)$ für alle $(v, \varphi) \in V \times V^*$ eine quadratische Form ist, müssen wir eine Bilinearform $\beta \colon (V \times V^*) \times (V \times V^*) \to K$ angeben, so dass $q$ die zu $\beta$ gehörige quadratische Form ist, dass also $q(v,\varphi) = \beta((v, \varphi), (v, \varphi))$ für alle $(v,\varphi) \in V \times V^*$.
Da $\ringchar K \neq 2$ gibt höchstens eine solche Bilinearform, und wir können diese durch eine Polarisationsformal angeben:

Für alle $(v_1, \varphi_1), (v_2, \varphi_2) \in V \times V^*$ definieren wir
\begin{align*}
              \beta((v_1, \varphi_1), (v_2, \varphi_2))
  &\coloneqq  \frac{q((v_1, \varphi_1) + (v_2, \varphi_2)) - q(v_1, \varphi_1) - q(v_2, \varphi_2)}{2}  \\
  &=          \frac{q(v_1 + v_2, \varphi_1 + \varphi_2) - q(v_1, \varphi_1) - q(v_2, \varphi_2)}{2}  \\
  &=          \frac{(\varphi_1 + \varphi_2)(v_1 + v_2) - \varphi_1(v_1) - \varphi_2(v_2)}{2}
   =          \frac{\varphi_1(v_2) + \varphi_2(v_1)}{2}.
\end{align*}
Eine einfache Rechnung zeigt, dass $\beta \colon (V \times V^*) \times (V \times V^*) \to K$ eine Bilinearform ist.
Für alle $(v, \varphi) \in V \times V^*$ ist
\[
    \beta((v, \varphi), (v, \varphi))
  = \frac{\varphi(v) + \varphi(v)}{2}
  = \varphi(v)
  = q(v, \varphi),
\]
also ist $q$ die zu $\beta$ gehörige quadratische Form.

Um zu zeigen, dass $q$ nicht-entartet ist, müssen wir zeigen, dass $\beta$ nicht-entartet.
Hierfür fixieren wir $(v, \varphi) \in V \times V^*$ mit $(v, \varphi) \neq (0,0)$ und geben ein $(w, \psi) \in V \times V^*$ an, so dass $\beta((v, \varphi), (w, \psi)) \neq 0$.
Dabei unterscheiden wir zwischen den beiden Fällen $v \neq 0$ und $\varphi \neq 0$.

Ist $v \neq 0$, so können wir $v_1 \coloneqq v$ zu einer Basis $\mc{B} = (v_1, \dotsc, v_n)$ von $V$ ergänzen (hier nutzen wir, dass $V$ endlichdimensional ist).
Für die entsprechende duale Basis $\mc{B}^* = (v_1^*, \dotsc, v_n^*)$ von $V^*$ gilt $v_i^*(v_j) = \delta_{ij}$ für alle $i,j = 1, \dotsc, n$.
Deshalb ist
\[
        \beta((v, \varphi), (0, v_1^*))
  =     \frac{v_1^*(v) + \varphi(0)}{2}
  =     \frac{v_1^*(v_1)}{2}
  =     \frac{1}{2}
  \neq  0.
\]
Ist andererseits $\varphi \neq 0$, so gibt es ein $w \in V$ mit $\varphi(w) \neq 0$, weshalb
\[
    \beta((v, \varphi), (w,0))
  = \frac{\varphi(w)}{2}
  \neq 0.
\]
Insgesamt zeigt dies, dass $\beta$ nicht-entartet ist.










\section{}





\subsection{Vorbereitung und hilfreiche Aussagen}


Wir wollen die gegebenen Probleme und Aussagen über quadratischen Formen auf Probleme und Aussagen über die assoziierte symmetrische Bilinearform zurückführen.


Zunächst beginnen wir aber mit dem allgemeinen Verhalten einer nicht-entartete symmetrischen Bilinearformen im Endlichdimensionalen.


\begin{proposition}\label{prop: orthogonal decomposition for nondegenerate symmetric forms}
  Es sei $V$ ein endlichdimensionaler $K$-Vektorraum, wobei $K$ ein beliebiger Körper ist, und $\beta \colon V \times V \to K$ eine nicht-entartete Bilinearform.
  Es sei $U \subseteq V$ ein beliebiger Untervektorraum.
  \begin{enumerate}[leftmargin=*]
    \item
      Die Einschränkungsabbildung $\rho \colon V^* \to U^*$, $\varphi \mapsto \varphi|_U$ ist surjektiv.
    \item
      Es gilt $\dim V = \dim U + \dim U^\perp$.
    \item
      Es gilt $(U^\perp)^\perp = U$.
    \item
      Die folgenden Bedingungen sind äquivalent:
      \begin{enumerate}
        \item
          Es gilt $V = U \oplus U^\perp$.
        \item
          Es gilt $U \cap U^\perp = 0$.
        \item
          Die Einschränkung $\beta|_{U \times U}$ ist nicht-entartet.
        \item
          Die Einschränkung $\beta|_{U^\perp \times U^\perp}$ ist nicht-entartet.
      \end{enumerate}
  \end{enumerate}
\end{proposition}
\begin{proof}
  \begin{enumerate}[leftmargin=*]
    \item
      Es sei $(v_1, \dotsc, v_m)$ eine Basis von $U$, und $(v_1, \dotsc, v_m, v_{m+1}, \dotsc, v_n)$ eine ergänzte Basis von $V$.
      Ist $\psi \in U^*$, also $\psi \colon U \to K$ eine lineare Abbildung, so gibt es genau eine lineare Abbildung $\varphi \colon V \to K$, also genau ein $\varphi \in V^*$, so dass $\varphi(v_i) = \psi(v_i)$ für alle $i = 1, \dotsc, m$ und $\varphi(v_j) = 0$ für alle $j = m+1, \dotsc, n$.
      Aufgrund der Linearität von $\psi$ und $\varphi$ ist bereits $\varphi(u) = \psi(u)$ für alle $u \in U$ und somit $\varphi|_U = \psi$.
    \item
      Da $\beta$ nicht-entartet ist, ist die Abbildung
      \[
        \Phi \colon V \to V^*,
        \quad
        v \mapsto \beta(v, -)
      \]
      injektiv.
      Wegen der Endlichdimensionalität von $V$ ist auch $V^*$ endlichdimensional mit $\dim V^* = \dim V$.
      Deshalb ist $\Phi$ bereits ein Isomorphismus, also auch surjektiv.
      Da die Einschränkungsabbildung $\rho \colon V^* \to U^*$, $\varphi \mapsto \varphi|_U$ surjektiv ist, ist auch die Komposition
      \[
        \Psi \coloneqq \rho \circ \Phi \colon V \to U^*,
        \quad
        v \mapsto \beta(v, -)|_U
      \]
      surjektiv.
      Es gilt $\ker \Psi = U^\perp$ und deshalb nach der Dimensionsformel
      \[
        \dim V
        = \dim \im \Psi + \dim \ker \Psi
        = \dim U^* + \dim U^\perp
        = \dim U + \dim U^\perp,
      \]
      wobei $\dim U^* = \dim U$, da $U$ endlichdimensional ist.
    \item
      Es gilt $U \subseteq (U^\perp)^\perp$, und da $\dim (U^\perp)^\perp = \dim V - \dim U^\perp = \dim U$ gilt dabei bereits die Gleichheit $U = (U^\perp)^\perp$.
    \item
      Es gilt genau dann $V = U \oplus U^\perp$, wenn $U + U^\perp = V$ und $U \cap U^\perp = 0$.
      Wegen der Gleichheit $\dim V = \dim U + \dim U^\perp$ ist dabei
      \[
          \dim( U + U^\perp )
        = \dim U + \dim U^\perp - \dim( U \cap U^\perp )
        = \dim V - \dim (U \cap U^\perp ),
      \]
      und deshalb genau dann $U + U^\perp = V$, wenn $U \cap U^\perp = 0$.
      Es gilt also genau dann $V = U \oplus U^\perp$, wenn $U \cap U^\perp = 0$.
      
      Für eine weitere Äquivalenz sei
      \[
          \rad(\beta|_{U \times U})
        = \{ u \in U \mid \text{$\beta(u, u') = 0$ für alle $u' \in U$} \}
      \]
      das Radikal von $\beta|_{U \times U}$.
      Die Einschränkung $\beta|_{U \times U}$ ist genau dann nicht-entartet, wenn $\rad(\beta|_{U \times U}) = 0$.
      Das Radikal lässt sich auch als $\rad(\beta|_{U \times U}) = U \cap U^\perp$ ausdrücken.
      Also ist $\beta|_{U \times U}$ genau dann nicht-entartet, wenn $U \cap U^\perp = 0$.
      
      Analog zur obigen Argumentation ergibt sich, dass $\beta|_{U^\perp, U^\perp}$ genau dann nicht-entartet ist, wenn $\rad(\beta|_{U^\perp \times U^\perp}) \neq 0$.
      Dabei ergibt sich nun, dass
      \[
          \rad(\beta|_{U^\perp \times U^\perp})
        = U^\perp \cap (U^\perp)^\perp
        = U^\perp \cap U
        = U \cap U^\perp
        = \rad(\beta|_{U \times U}).
      \]
      Deshalb ist genau dann $\rad(\beta|_{U^\perp \times U^\perp}) = 0$, wenn $\rad(\beta|_{U \times U}) = 0$, wenn also $\beta|_{U \times U}$ nicht-entartet ist.
    \qedhere
  \end{enumerate}
\end{proof}


Wir zeigen nun, dass es aufgrund der schönen Eigenschaften des Körpers ($\ringchar K \neq 2$ und $K$ ist quadratisch abgeschlossen) für eine nicht-entartete symmetrische Bilinearform auf einem endlichdimensionalen $K$-Vektorraum immer eine Orthonormalbasis gibt.


\begin{remark}
  Ein Körper $K$ heißt \emph{quadratisch abgeschlossen}, wenn es für jedes $a \in K$ ein $x \in K$ mit $a = x^2$ gibt.
  In anderen Worten:
  Jedes Element hat eine Quadratwurzel.
  
  Der Körper $\Rbb$ ist beispielweise nicht quadratisch abgeschlossen, da negative reelle Zahlen keine reelle Quadratwurzeln besitzen.
  
  Der Körper $\Cbb$ hingegen ist quadratisch abgeschlossen.
  Allgemeiner ist jeder algebraisch abgeschlossene Körper $K$ auch quadratisch abgeschlossen, denn für jedes $a \in K$ besitzt das Polynom $X^2 - a \in K[X]$ eine Nullstelle.
  
  Es ist allerdings nicht jeder quadratisch abgeschlossener Körper auch algebraisch abgeschlossen.
\end{remark}



\begin{theorem}\label{thrm: existence of orthonormalbases}
  Es sei $K$ ein Körper mit $\ringchar K \neq 2$, $V$ ein endlichdimensionaler $K$-Vektorraum und $\beta \colon V \times V \to K$ eine nicht-entartete symmetrische Bilinearform.
  \begin{enumerate}[leftmargin=*]
    \item
      Es gibt eine Basis $\mc{B} = (v_1, \dotsc, v_n)$ von $V$ und Skalare $\lambda_1, \dotsc, \lambda_n \in K$ mit $\lambda_i \neq 0$ für alle $i = 1, \dotsc n$, so dass
      \[
        \Mat_\mc{B}(\beta)
        =
        \begin{pmatrix}
          \lambda_1 &         &             \\
                    & \ddots  &             \\
                    &         & \lambda_n
        \end{pmatrix}.
      \]
    \item
      Ist $K$ zusätzlich quadratisch abgeschlossen, so kann die Basis $\mc{B}$ so gewählt werden, dass $\lambda_1 = \dotsb = \lambda_n = 1$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \begin{enumerate}[leftmargin=*]
    \item
      Wir zeigen die Aussage per Induktion über $\dim V$:
      Für $V = 0$ ist nichts zu zeigen.
      Es sei also $\dim V > 0$ und die Aussage gelte für alle kleineren Dimensionen.
      
      \begin{claim}
        Es gibt ein $v \in V$ mit $\beta(v,v) \neq 0$.
      \end{claim}
      \begin{proof}
        Es sei $q$ die zu $\beta$ gehörige quadratische Form.
        Da $V \neq 0$ gibt es ein $u \in V$ mit $u \neq 0$, und da $\beta$ nicht-entartet ist, gibt es ein $w \in V$ mit $\beta(u,w) \neq 0$.
        Da $\ringchar K \neq 2$ können wir eine Polarisationsformel nutzen um
        \[
                0 
          \neq  \beta(u,w)
          =     \frac{q(u+w)-q(u)-q(w)}{2}
        \]
        zu erhalten.
        Also ist $q(u+w) \neq 0$, $q(u) \neq 0$ oder $q(w) \neq 0$.
      \end{proof}
    
      Wir wählen nun $v_1 \in V$ mit $\beta(v_1,v_1) \neq 0$.
      Die Einschränkung $\beta|_{\Ell(v_1) \times \Ell(v_1)}$ ist nicht-entartet, denn ist $v \in \Ell(v_1)$ mit $v \neq 0$, so gibt es ein $\lambda \in K$ mit $\lambda \neq 0$ und $v = \lambda v_1$, weshalb
      \[
              \beta(v, v_1)
        =     \beta(\lambda v_1, v_1)
        =     \lambda \beta(v_1, v_1)
        \neq  0.
      \]
      Nach Proposition~\ref{prop: orthogonal decomposition for nondegenerate symmetric forms} ist deshalb $V = \Ell(v_1) \oplus U$ für $U \coloneqq \Ell(v_1)^\perp$, und die Einschränkung $\beta|_{U^ \times U}$ nicht-entartet.
      Nach Induktionsvoraussetzung gibt es eine Basis $\mc{C} = (v_2, \dotsc, v_n)$ von $U$ und Skalare $\lambda_2, \dotsc, \lambda_n \in K$ mit $\lambda_i \neq 0$ für alle $i = 2, \dotsc, n$, so dass
      \[
        \Mat_\mc{C}(\beta|_{U \times U})
        =
        \begin{pmatrix}
          \lambda_2 &         &           \\
                    & \ddots  &           \\
                    &         & \lambda_n,
        \end{pmatrix}.
      \]
      Da $V = \Ell(v_1) \oplus U$ ist $\mc{B} \coloneqq (v_1, v_2, \dotsc, v_n)$ eine Basis von $V$, und da die Summe orthogonal ist, ergibt sich mit $\lambda_1 \coloneqq \beta(v_1, v_1)$, dass
      \[
        \Mat_\mc{B}(\beta)
        =
        \begin{pmatrix}
          \lambda_1 &         &           \\
                    & \ddots  &           \\
                    &         & \lambda_n,
        \end{pmatrix}.
      \]
      
    \item
      Es sei $\mc{C} = (v_1, \dotsc, v_n)$ eine Basis von $V$, so dass $\Mat_\mc{C}(\beta)$ in Diagonalgestalt mit Diagonaleinträgen $\lambda_1, \dotsc, \lambda_n \in K$ ist, wobei $\lambda_i \neq 0$ für alle $i = 1, \dotsc, n$.
      Da $K$ quadratisch abgeschlossen ist, gibt es für jedes $i = 1, \dotsc, n$ einen Skalar $\mu_i \in K$ mit $\mu_i^2 = \lambda_i$; man beachte, dass $\mu_i \neq 0$ für alle $i = 1, \dotsc, n$.
      Die Basis $\mc{B} \coloneqq (\mu_1^{-1} v_1, \dotsc, \mu_n^{-1} v_n)$ leistet nun das Gewünschte.
    \qedhere
  \end{enumerate}
\end{proof}


\begin{definition}
  Es sei $V$ ein $K$-Vektorraum und $\beta \colon V \times V \to K$ eine symmetrische Bilinearform.
  Eine Basis $(v_i)_{i \in I}$ von $V$ heißt \emph{orthonormal} bezüglich $\beta$, falls $\beta(v_i, v_j) = \delta_{ij}$ für alle $i,j \in I$.
\end{definition}


\begin{remark}
  \begin{enumerate}[leftmargin=*]
    \item
      Besitzt ein $K$-Vektorraum $V$ bezüglich einer symmetrischen Bilinearform $\beta \colon V \times V \to K$ eine Orthonormalbasis $(v_i)_{i \in I}$, so ist $\beta$ nicht-entartet.
      Ist nämlich $v \in V$ mit $\beta(v, v') = 0$ für alle $v' \in V$, so lässt sich $v$ als Linearkombination $v = \sum_{i \in I} \lambda_i v_i$ darstellen, und da
      \[
          \lambda_j
        = \sum_{i \in I} \lambda_i \delta_{ij}
        = \sum_{i \in I} \lambda_i \beta(v_i, v_j)
        = \beta\left( \sum_{i \in I} \lambda_i v_i, v_j \right)
        = \beta(v, v_j)
        = 0
        \quad
        \text{für alle $j \in I$}
      \]
      ist bereits $v = 0$.
    \item
      Ist $(v_i)_{i \in I}$ eine Orthonormalbasis von $V$ bezüglich einer symmetrischen Bilinearform $\beta \colon V \times V \to K$, so gilt für die zugehörige quadratische Form $q$ und einen Vektor $v \in V$ mit Linearkombination $v = \sum_{i \in I} \lambda_i v_i$, dass
      \begin{align*}
            q(v)
         =  \beta(v,v)
        &=  \beta\left( \sum_{i \in I} \lambda_i v_i, \sum_{j \in I} \lambda_j v_j \right)  \\
        &=  \sum_{i,j \in I} \lambda_i \lambda_j \beta(v_i, v_j)
         =  \sum_{i,j \in I} \lambda_i \lambda_j \delta_{ij}
         =  \sum_{i \in I} \lambda_i^2.
      \end{align*}
      Dabei wird für die dritte Gleichheit die Bilinaerität von $\beta$ genutzt.
  \end{enumerate}
\end{remark}


Um quadratische Aussagen in bilineare zu übersetzen, nutzen wir das folgende Lemma:


\begin{lemma}\label{lem: homomorphism of quadratic and bilinear spaces}
  Es sei $K$ ein Körper mit $\ringchar K \neq 2$.
  Es seien $V$ und $W$ zwei $K$-Vektorräume.
  Es seien $\beta \colon V \times V \to K$ und $\gamma \colon W \times W \to K$ zwei Bilinearformen, und $q \colon V \to K$ und $r \colon W \to K$ seien die assoziierten quadratischen Formen.
  Für eine lineare Abbildung $f \colon V \to W$ sind die folgenden beiden Bedingungen äquivalent:
  \begin{enumerate}
    \item
      Es ist $r(f(v)) = q(v)$ für alle $v \in V$.
    \item
      Es ist $\gamma(f(v_1),f(v_2)) = \beta(v_1, v_2)$ für alle $v_1, v_2 \in V$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Angenommen, es ist $r(f(v)) = q(v)$ für alle $v \in V$.
  Nach einer Polarisationsformel ist dann für alle $v_1, v_2 \in V$ auch
  \begin{align*}
        \gamma(f(v_1), f(v_2))
    &=  \frac{r(f(v_1) + f(v_2)) - r(f(v_1)) - r(f(v_2))}{2}  \\
    &=  \frac{r(f(v_1 + v_2)) - r(f(v_1)) - r(f(v_2))}{2}     \\
    &=  \frac{q(v_1 + v_2) - q(v_1) - q(v_2)}{2}
    =   \beta(v_1, v_2).
  \end{align*}
  Gilt andererseits $\gamma(f(v_1), f(v_2)) = \beta(v_1, v_2)$ für alle $v_1, v_2 \in V$, so ergibt sich für alle $v \in V$, dass
  $ r(f(v)) = \gamma(f(v), f(v)) = \beta(v, v) = q(v)$.
\end{proof}





\subsection{Der geradedimensionale Fall}


Zunächst sei $X$ ein endlichdimensionaler $K$-Vektorraum, wobei $\ringchar K \neq 2$ und $K$ quadratisch abgeschlossen ist, so dass $\dim X$ gerade ist, und $q \colon X \to K$ eine nicht-entartete quadratische Form.
Es sei $\beta \colon X \times X \to K$ die zu $q$ gehörige Bilinearform.
Dass $q$ nicht-entartet ist, bedeutet gerade, dass $\beta$ nicht-entartet ist.
Nach Theorem~\ref{thrm: existence of orthonormalbases} gibt es eine Orthonormalbasis $(x_1, \dotsc, x_{2n})$ von $X$ bezüglich $\beta$.

Es sei $V$ ein beliebiger $n$-dimensionaler $K$-Vektorraum.
Wie in Aufgabe~3 gesehen ist $r \colon V \times V^* \to K$, $(v,\varphi) \mapsto \varphi(v)$ die quadratische Form einer nicht-entarteten symmetrischen Bilinearform $\gamma \colon (V \times V^*) \times (V \times V^*) \to K$.
Nach Theorem~\ref{thrm: existence of orthonormalbases} gibt es eine Orthonormalbasis $(v_1, \dotsc, v_{2n})$ von $V \times V^*$ bezüglich $\gamma$.

Es gibt nun einen eindeutigen Isomorphismus $\Phi \colon X \to V \times V^*$ mit $\Phi(x_i) = v_i$ für alle $i = 1, \dotsc, 2n$.
Es gilt
\[
  \gamma(\Phi(x_i), \Phi(x_j)) = \gamma(v_i, v_j) = \delta_{ij} = \beta(x_i, x_j)
  \quad
  \text{für alle $i, j = 1, \dotsc, n$}.
\]
Wegen der Linearität von $\Phi$ und Bilinearität von $\beta$ und $\gamma$ ergibt sich daraus, dass bereits $\gamma(\Phi(y_1), \Phi(y_2)) = \beta(y_1, y_2)$ für alle $y_1, y_2 \in X$.
Nach Lemma~\ref{lem: homomorphism of quadratic and bilinear spaces} ist deshalb $r(\Phi(y)) = q(y)$ für alle $y \in X$.
Also ist $\Phi$ ein Isomorphismus von quadratischen Räumen bezüglich $q$ und $r$.





\subsection{Der ungeradedimensionale Fall}

Es sei nun $X$ ein endlichdimensionaler $K$-Vektorraum, so dass $\dim X$ ungerade ist, und $q$ eine nicht-entartete quadratische Form auf $X$, sowie $\beta$ die zu $q$ assoziierte symmetrische Bilinearform auf $X$.
Nach Theorem~\ref{thrm: existence of orthonormalbases} gibt es eine Orthonormalbasis $(x_1, \dotsc, x_{2n+1})$ von $X$.

Ist $V$ ein $n$-dimensionaler $K$-Vektorraum und $r \colon V \times V^* \to K$, $(v, \varphi) \mapsto \varphi(v)$, so gibt es nach Aufgabe~2 und Theorem~\ref{thrm: existence of orthonormalbases} eine Orthonormalbasis $\mc{B} = ((v_1, \varphi_1), \dotsc, (v_{2n}, \varphi_{2n}))$ von $V \times V^*$ bezüglich der zu $r$ gehörigen symmetrischen Bilinearform $\gamma$.

Dabei haben wir in Aufgabe~2 durch Interpolation gesehen, dass
\[
    \gamma((w_1, \psi_1), (w_2, \psi_2))
  = \frac{\psi_1(w_2) + \psi_2(w_1)}{2}
  \quad
  \text{für alle $(w_1, \psi_1), (w_2, \psi_2) \in V \times V^*$}.
\]
Durch Interpolation ergibt sich analog zu Aufgabe~2, dass die zu $\hat{r} \colon V \times V^* \times K \to K$, $(v, \varphi, \lambda) \mapsto \varphi(v) + \lambda^2 = r(v, \varphi) + \lambda^2$ gehörige symmetrische Bilinearform $\hat{\gamma}$ durch
\[
    \hat{\gamma}(w_1, \psi_1, \mu_1), (w_2, \psi_2, \mu_2))
  = \frac{\psi_1(w_2) + \psi_2(w_1)}{2} + \mu_1 \mu_2
  = \gamma((w_1, \psi_1), (w_2, \psi_2)) + \mu_1 \mu_2
\]
gegeben ist.

Durch den Isomorphismus $((V \times V^*) \times K) \to V \times V^* \times K$, $((v, \varphi), \lambda) \mapsto (v, \varphi, \lambda)$ ergibt sich aus der Basis $\mc{B}$ von $V \times V^*$ die Basis $\mc{C} \coloneqq ((v_1, \varphi_1, 0), \dotsc, (v_{2n}, \varphi_{2n}, 0), (0,0,1))$ von $V \times V^* \times K$.

Dies ist eine Orthonormalbasis von $V \times V^* \times K$ bezüglich $\hat{\gamma}$:
Da
\[
    \hat{\gamma}((v_i, \varphi_i, 0), (v_j, \varphi_j, 0))
  = \underbrace{ \gamma((v_i, \varphi_i, (v_j, \varphi_j)) }_{= \delta_{ij}} + 0^2
  = \delta_{ij}
  \quad
  \text{für alle $i,j = 1, \dotsc, 2n$}
\]
ist die Familie $((v_1, \varphi_1,0), \dotsc, (v_{2n}, \varphi_{2n}, 0))$ orthonormal bezüglich $\hat{\gamma}$.
Dass $(0,0,1)$ bezüglich $\hat{\gamma}$ orthogonal zu den üblichen $2n$ Basiselementen ist, ergibt sich daraus, dass
\[
  \hat{\gamma}((v,\varphi,0), (0,0,1))
  = \gamma((v, \varphi), (0,0)) + 0 \cdot 1
  = 0 + 0
  = 0
  \quad
  \text{für alle $(v, \varphi) \in V \times V^*$},
\]
und die Normiertheit von $(0,0,1)$ bezüglich $\hat{\gamma}$ ergibt sich durch direktes Nachrechnen:
\[
  \hat{\gamma}((0,0,1), (0,0,1))
  = \gamma((0,0), (0,0)) + 1 \cdot 1
  = 0 + 1
  = 1.
\]

Für den eindeutige Isomorphismus $\Phi \colon X \to V \times V^* \times K$ mit $\Phi(x_i) = (v_i, \varphi_i, 0)$ für alle $i = 1, \dotsc, 2n$ und $\Phi(x_{2n+1}) = (0,0,1)$ ergibt sich nun analog zum geradedimensionalen Fall zunächst, dass $\gamma(\Phi(y_1), \Phi(y_2)) = \beta(y_1, y_2)$ für alle $y_1, y_2 \in X$, und daraus dann mithilfe von Lemma~\ref{lem: homomorphism of quadratic and bilinear spaces}, dass $\Phi$ ein Isomorphismus von quadratischen Räumen ist.










\section{}


Die nicht-entartete symmmetrische Bilinearform $\beta$ korrespondiert wegen der Endlichdimensionalität von $V$ zu einem Isomorphismus
\[
  B \colon V \to V^*,
  \quad
  v \mapsto \beta(-, v).
\]
Dass ein Endomorphismus $g \colon V \to V$ bezüglich $\beta$ adjungiert zu einem Endomorphismus $f \colon V \to V$ ist, dass also
\[
    \beta(f(v_1), v_2)
  = \beta(v_1, g(v_2))
  \quad
  \text{für alle $v_1, v_2 \in V$},
\]
ist äquivalent dazu, dass
\[
    B(v_2)(f(v_1))
  = B(g(v_2))(v_1)
  \quad
  \text{für alle $v_1, v_2 \in V$}.
\]
Mithilfe der zu $f$ dualen Abbildung $f^T \colon V^* \to V^*$, $\varphi \mapsto \varphi \circ f$ lässt sich diese Bedingung zu
\begin{gather*}
    f^T(B(v_2))(v_1)
  = B(g(v_2))(v_1)
  \quad
  \text{für alle $v_1, v_2 \in V$}
\shortintertext{umschreiben, also zu}
    f^T(B(v_2))
  = B(g(v_2))
  \quad
  \text{für alle $v_2 \in V$},
\end{gather*}
und somit zu $f^T \circ B = B \circ g$.
Also muss $g = B^{-1} f^T B$, was Existenz und Eindeutigkeit zeigt.

Für alle $v_1, v_2 \in V$ gilt
\[
    \beta(f^*(v_1), v_2)
  = \beta(v_2, f^*(v_1))
  = \beta(f(v_2), v_1)
  = \beta(v_1, f(v_2)),
\]
also erfüllt $f$ die definierende Eigenschaft von $f^{**}$, weshalb $f^{**} = f$.










\section{}


Es sei
\[
  I \colon V \to V^{**},
  \quad
  v \mapsto e_v
  \quad\text{mit}\quad
  e_v(\varphi) = \varphi(v)
  \quad
  \text{für alle $\varphi \in V^*$}
\]
der natürliche Isomorphismus von $V$ nach $V^{**}$.
Es sei $\beta \colon (V \times V^*) \times (V \times V^*) \to K$ die symmetrische Bilinearform aus Aufgabe~3, d.h.\ es ist
\[
    \beta((v_1, \varphi_1), (v_2, \varphi_2))
  = \frac{ \varphi_1(v_2) + \varphi_2(v_1) }{2}
  \quad
  \text{für alle $(v_1, \varphi_1), (v_2, \varphi_2) \in V \times V^*$}.
\]
Für den Endomorphismus $f \colon V \times V^* \to V \times V^*$ mit
\[
  f
  =
  \begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix}
\]
mit $a \colon V \to V$, $b \colon V^* \to V$, $c \colon V \to V^*$, $d \colon V^* \to V^*$ gilt es zu zeigen, dass der Endomorphismus $g \colon V \times V^* \to V \times V^*$ mit
\[
  g
  =
  \begin{pmatrix}
    I^{-1} d^* I  & I^{-1} b^*  \\
    c^* I           & a^*.
  \end{pmatrix},
\]
bezüglich $\beta$ adjungiert zu $f$ ist.


\begin{remark}
  Auf dem Übungszettel wird die Identifikation $I$ nicht \emph{explizit} ausgeschrieben und genutzt, wir wollen dies hier aber tun.
\end{remark}


Für alle $(v_1, \varphi_1), (v_2, \varphi_2) \in V$ ist auf der einen Seite
\begin{align*}
      2\beta(f(v_1, \varphi_1), (v_2, \varphi_2))
  &=  2\beta\Bigl(
              \Bigl(
              \underbrace{a(v_1) + b(\varphi_1)}_{v'},
              \underbrace{c(v_1) + d(\varphi_1)}_{\varphi'}
              \Bigr),
              (v_2, \varphi_2)
            \Bigr)  \\
  &=  \underbrace{(c(v_1) + d(\varphi_1))}_{\varphi'}(v_2)
      + \varphi_2\Bigl( \underbrace{a(v_1) + b(\varphi_1)}_{v'} \Bigr)         \\
  &=  c(v_1)(v_2) + d(\varphi_1)(v_2) + \varphi_2(a(v_1)) + \varphi_2(b(\varphi_1))
\end{align*}
und auf der anderen Seite
\begin{align*}
   &\,  2\beta( (v_1, \varphi_1), g(v_2, \varphi_2) ) \\
  =&\,  2\beta\Bigl(
                (v_1, \varphi_1),
                \Bigl(
                  \underbrace{(I^{-1} d^* I)(v_2) + (I^{-1} b^*)(\varphi_2)}_{v'},
                  \underbrace{(c^* I)(v_2) + a^*(\varphi_2)}_{\varphi'}
                \Bigr)
              \Bigr) \\
  =&\,    \varphi_1\Bigl(
                      \underbrace{(I^{-1} d^* I)(v_2) + (I^{-1} b^*)(\varphi_2)}_{v'}
                    \Bigr)
        + \underbrace{( (c^* I)(v_2) + a^*(\varphi_2) )}_{\varphi'}(v_1)           \\
  =&\,  \varphi_1((I^{-1} d^* I)(v_2)) + \varphi_1((I^{-1} b^*)(\varphi_2)) + (c^* I)(v_2)(v_1) + a^*(\varphi_2)(v_1).
\end{align*}
Der einzige Unterschied zwischen den beiden Summen besteht darin, dass der Isomorphismus $I$ und das Dualisieren $(-)^*$ linearer Abbildungen genutzt wird, um die einzelnen Summanden umzuschreiben.
Dabei haben wir für alle $v \in V$ und $\varphi \in V^*$, dass
\begin{align*}
      \varphi( (I^{-1} d^* I)(v) )
  &=  \varphi( I^{-1}( d^*( I( v ) ) ) )
   =  \varphi( I^{-1}( d^*( e_v ) )
   =  \varphi( I^{-1}( e_v d ) )        \\
  &=  I(I^{-1}(e_v d))(\varphi)
   =  (e_v d)(\varphi)
   =  e_v(d(\varphi))
   =  d(\varphi)(v),
\end{align*}
wobei wir beim Übergang in die zweite Zeile nutzen, dass $\varphi(w) = e_w(\varphi) = I(w)(\varphi)$ mit $w = I^{-1}(e_v d)$.
Ähnlich erhalten wir für alle $\varphi, \psi \in V^*$, dass
\begin{align*}
      \varphi((I^{-1} b^*)(\psi))
  &=  \varphi( I^{-1}( b^*(\psi) ) )
   =  \varphi( I^{-1} (\psi b) )      \\
  &=  I( I^{-1}(\psi b) )(\varphi)
   =  (\psi b)(\varphi)
   =  \psi(b(\varphi)).
\end{align*}
Außerdem haben wir für alle $v, w \in V$, dass
\begin{gather*}
    (c^* I)(w)(v)
  = c^*(I(w))(v)
  = c^*(e_w)(v)
  = (e_w c)(v)
  = e_w(c(v))
  = c(v)(w),
\end{gather*}
und für alle $v \in V$ und $\varphi \in V^*$, dass
\[
    a^*(\varphi)(v)
  = (\varphi a)(v)
  = \varphi(a(v)).
\]
Damit sind insgesamt beide Summen gleich.










\section{}


Um zu Umgehen, dass Körper-Shenanigans wie die Charakteristik oder unendliche Dimension an den Gegenbeispielen Schuld sind, möchten wir die Gegenbeispiele für endlichdimensionalen $\Cbb$-Vektorräumen konstruieren; dabei beschränken wir uns o.B.d.A.\ auf $\Cbb^n$.

Nach Theorem~\ref{thrm: existence of orthonormalbases} wissen wir, dass jede nicht-entartete symmetrische Bilinearform auf $\Cbb^n$ eine Orthonormalbasis besitzt.
Es genügt daher, die Standardbilinearform
\[
  \beta \colon \Cbb^n \times \Cbb^n \to \Cbb,
  \quad\text{mit}\quad
    \beta(x,y)
  = x^T y
  = \sum_{i=1}^n x_i y_i
  \quad
  \text{für alle $x, y \in \Cbb^n$}
\]
zu betrachten.
Die Bilinearität ergibt sich durch einfaches Nachrechnen, und dass $\beta$ nicht entartet ist, ergibt sich daraus, dass die Standardbasis $(e_1, \dotsc, e_n)$ eine Orthonormalbasis bezüglich $\beta$ ist.

\begin{remark}
  Die Standardbilinearform ist \emph{nicht} mit dem Standardskalarprodukt zu verwechseln!
  Die Standardbilinearform ist $\Cbb$-bilinear, während das Standardskalarprodukt $\Cbb$-sesquilinear ist.
\end{remark}

Ist $f \colon \Cbb^n \to \Cbb^n$ ein Endomorphismus, so bezeichne im Folgenden $f^\ad \colon \Cbb^n \to \Cbb^n$ den $\beta$-adjungierten Endomorphismus zu $f$.
Ist $A \in \Mat_n(\Cbb)$ die eindeutige Matrix mit $f(x) = Ax$ für alle $x \in \Cbb^n$, so gilt für die eindeutige Matrix $A^\ad \in \Mat_n(\Cbb)$ mit $f^\ad(x) = A^\ad x$ für alle $x \in \Cbb^n$, dass
\[
    A^\ad_{ij}
  = \beta(e_i, A^\ad e_j)
  = \beta(e_i, f^\ad(e_j))
  = \beta(f(e_i), e_j)
  = \beta(A e_i, e_j)
  = A_{ji}
\]
für alle $i,j = 1, \dotsc, n$.
Es ist also $A^\ad = A^T$.
Die symmetrischen Matrizen entsprechen also genau den $\beta$-selbstadjungierten Endomorphismen, und die schiefsymmetrischen Matrizen entsprechen genau den $\beta$-schiefsymmetrischen Matrizen.

% Wir wollen auch normale Endomorphismen über ihre Matrizen beschreiben:
% Ist $f \colon \Cbb^n \to \Cbb^n$ ein beliebiger Endomorphismus, und $M \in \Mat_n(\Cbb)$ die eindeutige Matrix mit $f(x) = Mx$ für alle $x \in \Cbb^n$.
% 
% 
% \begin{lemma}
%   Ist $K$ ein Körper mit $\ringchar K \neq 2$, so gibt es für jede Matrix $M \in \Mat_n(K)$ eine eindeutige symmetrische Matrix $S \in \Mat_n(K)$ und eine eindeutige schiefsymmetrische Matrix $A \in \Mat_n(K)$, so dass $M = S + A$.
% \end{lemma}
% \begin{proof}
%   Die Abbildung
%   \[
%     e \colon \Mat_n(K) \to \Mat_n(K),
%     \quad
%     A \mapsto \frac{A + A^T}{2}
%   \]
%   ist linear und idempotent.
%   Deshalb ist $\Mat_n(K) = \im e \oplus \ker e$, und für den ersten Summanden gilt \mbox{$\im e = \{A \in \Mat_n(K) \mid e(A) = A\}$}.
%   Dass $e(A) = A$ ist äquivalent dazu, dass $A$ symmetrisch ist, also ist $\im e$ der Untervektorraum der symmetrischen Matrizen.
%   Dass $e(A)= 0$ ist äquivalent dazu, dass $A$ schiefsymmetrisch ist, also ist $\ker e$ der Untervektorraum der schiefsymmetrischen Matrizen.
% \end{proof}
% 
% 
% Nach dem Lemma können wir $M$ eindeutig als Summe $M = S + A$ schreiben, wobei $S \in \Mat_n(K)$ eine symmetrische Matrix und $A \in \Mat_n(K)$ eine schiefsymmetrische Matrix ist.
% Dass $f$ normal ist, also $f$ und $f^\ad$ kommutieren, ist äquivalent dazu, dass $M$ und $M^\ad = M^T$ kommutieren.
% Da $M^T = S - A$ ist
% \begin{gather*}
%     M M^T
%   = (S + A)(S - A)
%   = S^2 + AS - SA - A^2
% \shortintertext{und}
%     M^T M
%   = (S - A)(S + A)
%   = S^2 - AS + SA - A^2.
% \end{gather*}
% Also ist $f$ genau normal, wenn
% \[
%      AS - SA
%   = -AS + SA
%   = -(AS - SA)
% \]
% wenn also $AS - SA = 0$.
% Also ist $f$ genau dann normal, wenn $S$ und $A$, der symmetrische und schiefsymmetrische Anteil der entsprechenden Matrix $M$, kommutieren.





\subsection{}

Es genügt eine Matrix $A \in \Mat_2(\Cbb)$ mit $A \neq 0$ zu finden, die symmetrisch und nilpotent ist.
Man beachte, dass $A$ nicht reell seien darf, denn sonst wäre $A$ als reelle symmetrische Matrix diagonalisierbar, kann also wegen $A \neq 0$ nicht nilpotent sein.
Dass $A \neq 0$ nilpotent ist, ist äquivalent dazu, dass die Jordannormal von $A$ durch
\[
  \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
\]
gegeben ist, d.h.\ es gibt eine Matrix $S \in \GL_2(\Cbb)$ mit
\[
  A
  =
  S
  \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
  S^{-1}.
\]
Wir wollen also $S$ so wählen, dass die Matrix auf der rechten Seite symmetrisch ist.
Ist
\begin{gather*}
  S
  =
  \begin{pmatrix}
    a & b \\
    c & d
  \end{pmatrix},
\shortintertext{so ist}
  S^{-1}
  =
  \frac{1}{ad-bc}
  \begin{pmatrix}
     d  & -b  \\
    -c  &  a
  \end{pmatrix},
\shortintertext{und somit}
  A
  =
  S
  \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
  S^{-1}
  =
  \frac{1}{ad-bc}
  \begin{pmatrix}
    -ac   & a^2 \\
    -c^2  & ac
  \end{pmatrix}.
\end{gather*}
Damit die rechte Seite symmetrisch ist, wählen wir $c = i$ und $a = 1$.
Wir wählen außerdem $d = 1/2$ und $b = i/2$, damit $\det S = ad-bc = 1$.
Wir erhalten damit die symmetrische, nilpotente Matrix
\[
  A
  =
  \begin{pmatrix*}[r]
    -i & 1 \\
     1 & i
  \end{pmatrix*}.
\]
Damit ist $f \colon \Cbb^2 \to \Cbb^2$, $x \mapsto Ax$ ein nilpotenter, $\beta$-selbstadjungierter Endomorphismus mit $f \neq 0$.





\subsection{}

Hierfür habe ich noch kein Beispiel gefunden.





\subsection{}

Wir betrachten den Endomorphismus $f \colon \Cbb^2 \to \Cbb^2$, $x \mapsto Ax$ mit der Matrix
\[
  A
  \coloneqq
  \begin{pmatrix}
    0 & 1 \\
    0 & 0
  \end{pmatrix}
  \in
  \Mat_2(\Cbb).
\]
Also ist $f$ der Endomorphismus, der $e_2$ auf $e_1$ schiebt, und $e_1$ auf $0$.
Das Adjungierte $f^\ad$ ist durch Linksmultiplikation mit der Matrix
\[
    A^\ad
  = A^T
  =
  \begin{pmatrix}
    0 & 0 \\
    1 & 0
  \end{pmatrix}
\]
gegeben.
Also ist $f^\ad$ der Endomorphismus, der $e_1$ auf $e_2$ schiebt, und $e_2$ auf $0$.

(Man kann auch direkt sehen, dass $f^\ad$ so aussehen muss:
$f$ wirkt auf $(x_1, x_2) \in \Cbb^2$, indem es die Koordinaten nach rechts schiebt, d.h.\ es ist der Rechtshiftoperator.
Da $\beta$ durch eintragsweises Multiplizieren funktioniert, hat ein Rechtsshift in einem Argument von $\beta$ den gleichen Effekt wie ein Linksshift im anderen Argument.)

Betrachten man nun $U \coloneqq \Ell(e_1)$, so ist $U = \ker f$ invariant unter $f$.
Da $f^\ad(e_1) = e_2 \notin U$ ist $U$ aber nicht invariant unter $f^\ad$.
























\end{document}
